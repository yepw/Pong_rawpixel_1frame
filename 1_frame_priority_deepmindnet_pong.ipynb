{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pong_Raw_Pixel_1_Frame",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "uxJpWt_mPRPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "81c1ca56-033c-4afb-e2aa-4264be297884"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "!pip install gym\n",
        "!pip install gym[atari]\n",
        "!pip install torch\n",
        "!pip install torchvision\n",
        "!pip install tqdm\n",
        "!pip install Pillow\n",
        "!pip install opencv-python\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!pip install gym\\n!pip install gym[atari]\\n!pip install torch\\n!pip install torchvision\\n!pip install tqdm\\n!pip install Pillow\\n!pip install opencv-python\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "u1qEbpJ2PZ2g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import sys\n",
        "import gym\n",
        "from gym import spaces\n",
        "import torch\n",
        "import pylab\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms,models\n",
        "import cv2\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "319-wZ4YJWMr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "replay_buffer_size = 1000000\n",
        "frame_history_len = 4\n",
        "\n",
        "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TnSVqUVgPdBB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SumTree\n",
        "# a binary tree data structure where the parentâ€™s value is the sum of its children\n",
        "class SumTree:\n",
        "    write = 0\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.tree = np.zeros(2 * capacity - 1)\n",
        "        self.data = np.zeros(capacity, dtype=object)\n",
        "        self.n_entries = 0\n",
        "\n",
        "    # update to the root node\n",
        "    def _propagate(self, idx, change):\n",
        "        parent = (idx - 1) // 2\n",
        "\n",
        "        self.tree[parent] += change\n",
        "\n",
        "        if parent != 0:\n",
        "            self._propagate(parent, change)\n",
        "\n",
        "    # find sample on leaf node\n",
        "    def _retrieve(self, idx, s):\n",
        "        left = 2 * idx + 1\n",
        "        right = left + 1\n",
        "\n",
        "        if left >= len(self.tree):\n",
        "            return idx\n",
        "\n",
        "        if s <= self.tree[left]:\n",
        "            return self._retrieve(left, s)\n",
        "        else:\n",
        "            return self._retrieve(right, s - self.tree[left])\n",
        "\n",
        "    def total(self):\n",
        "        return self.tree[0]\n",
        "\n",
        "    # store priority and sample\n",
        "    def add(self, p, data):\n",
        "        idx = self.write + self.capacity - 1\n",
        "\n",
        "        self.data[self.write] = data\n",
        "        self.update(idx, p)\n",
        "\n",
        "        self.write += 1\n",
        "        if self.write >= self.capacity:\n",
        "            self.write = 0\n",
        "\n",
        "        if self.n_entries < self.capacity:\n",
        "            self.n_entries += 1\n",
        "\n",
        "    # update priority\n",
        "    def update(self, idx, p):\n",
        "        change = p - self.tree[idx]\n",
        "\n",
        "        self.tree[idx] = p\n",
        "        self._propagate(idx, change)\n",
        "\n",
        "    # get priority and sample\n",
        "    def get(self, s):\n",
        "        idx = self._retrieve(0, s)\n",
        "        dataIdx = idx - self.capacity + 1\n",
        "\n",
        "        return (idx, self.tree[idx], self.data[dataIdx])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3QvOczUePjk0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Memory:  # stored as ( s, a, r, s_ ) in SumTree\n",
        "    e = 0.01\n",
        "    a = 0.6\n",
        "    beta = 0.4\n",
        "    beta_increment_per_sampling = 0.001\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.tree = SumTree(capacity)\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def _get_priority(self, error):\n",
        "        return (error + self.e) ** self.a\n",
        "\n",
        "    def add(self, error, sample):\n",
        "        p = self._get_priority(error)\n",
        "        self.tree.add(p, sample)\n",
        "\n",
        "    def sample(self, n):\n",
        "        batch = []\n",
        "        idxs = []\n",
        "        segment = self.tree.total() / n\n",
        "        priorities = []\n",
        "\n",
        "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
        "\n",
        "        for i in range(n):\n",
        "            a = segment * i\n",
        "            b = segment * (i + 1)\n",
        "\n",
        "            s = random.uniform(a, b)\n",
        "            (idx, p, data) = self.tree.get(s)\n",
        "            priorities.append(p)\n",
        "            batch.append(data)\n",
        "            idxs.append(idx)\n",
        "\n",
        "        sampling_probabilities = priorities / self.tree.total()\n",
        "        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)\n",
        "        is_weight /= is_weight.max()\n",
        "\n",
        "        return batch, idxs, is_weight\n",
        "\n",
        "    def update(self, idx, error):\n",
        "        p = self._get_priority(error)\n",
        "        self.tree.update(idx, p)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UGreBRhKLziu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "153c40bd-f59a-48ca-9f16-fdfe4779b987"
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, action_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "'''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass DQN(nn.Module):\\n    def __init__(self, state_size, action_size):\\n        super(DQN, self).__init__()\\n        self.fc = nn.Sequential(\\n            nn.Linear(state_size, 128),\\n            nn.ReLU(),\\n            nn.Linear(128, 64),\\n            nn.ReLU(),\\n            nn.Linear(64, 32),\\n            nn.ReLU(),\\n            nn.Linear(32, action_size)\\n        )\\n\\n    def forward(self, x):\\n        return self.fc(x)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "8-ZfIJJJJOKw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self,  action_size=6, in_channels=1):\n",
        "        \"\"\"\n",
        "        Initialize a deep Q-learning network as described in\n",
        "        https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
        "        Arguments:\n",
        "            in_channels: number of channel of input.\n",
        "                i.e The number of most recent frames stacked together as describe in the paper\n",
        "            action_size: number of action-value to output, one-to-one correspondence to action in game.\n",
        "        \"\"\"\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
        "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
        "        self.fc5 = nn.Linear(512, action_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
        "        return self.fc5(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IUaeUx2tPxWX",
        "colab_type": "code",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "158c1121-8248-48fb-c375-3145661375b3"
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "'''\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(state_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, action_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "'''"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nclass DQN(nn.Module):\\n    def __init__(self, state_size, action_size):\\n        super(DQN, self).__init__()\\n        self.fc = nn.Sequential(\\n            nn.Linear(state_size, 256),\\n            nn.ReLU(),\\n            nn.Linear(256, 128),\\n            nn.ReLU(),\\n            nn.Linear(128, 64),\\n            nn.ReLU(),\\n            nn.Linear(64, action_size)\\n        )\\n\\n    def forward(self, x):\\n        return self.fc(x)\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "metadata": {
        "id": "7OuaUi6cPx4h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "EPISODES = 500*10000\n",
        "\n",
        "# DQN Agent for the Cartpole\n",
        "# it uses Neural Network to approximate q function\n",
        "# and prioritized experience replay memory & target q network\n",
        "class DQNAgent():\n",
        "    def __init__(self, action_size):\n",
        "        # if you want to see Cartpole learning, then change to True\n",
        "        self.render = False\n",
        "        self.load_model = False\n",
        "\n",
        "        # get size of state and action\n",
        "        #self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "\n",
        "        # These are hyper parameters for the DQN\n",
        "        self.discount_factor = 0.99\n",
        "        self.learning_rate = 0.00025\n",
        "        self.memory_size = 100000 #20000*50//2\n",
        "        self.epsilon =  1.0\n",
        "        self.epsilon_min = 0.01*5\n",
        "        self.explore_step =100000 #50000*5 #5000*20\n",
        "        self.epsilon_decay = (self.epsilon - self.epsilon_min) / self.explore_step\n",
        "        self.batch_size = 32\n",
        "        self.train_start = 50000\n",
        "\n",
        "        # create prioritized replay memory using SumTree\n",
        "        self.memory = Memory(self.memory_size)    \n",
        "        \n",
        "        # create main model and target model\n",
        "        self.model = DQN( action_size).to(device)\n",
        "        self.model.apply(self.weights_init)\n",
        "        self.target_model = DQN( action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(),\n",
        "                                    lr=self.learning_rate)\n",
        "\n",
        "        # initialize target model\n",
        "        self.update_target_model()\n",
        "\n",
        "        if self.load_model:\n",
        "            self.model = torch.load('save_model/cartpole_dqn')\n",
        "\n",
        "    # weight xavier initialize\n",
        "    def weights_init(self, m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Linear') != -1:\n",
        "            torch.nn.init.xavier_uniform(m.weight)\n",
        "\n",
        "    # after some time interval update the target model to be same with model\n",
        "    def update_target_model(self):\n",
        "        self.target_model.load_state_dict(self.model.state_dict())\n",
        "\n",
        "    # get action from model using epsilon-greedy policy\n",
        "    def get_action(self, state):\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        else:\n",
        "            state = torch.from_numpy(state).float().to(device)\n",
        "            #state = torch.FloatTensor(state).to(device)\n",
        "            q_value = self.model(state)\n",
        "            _, action = torch.max(q_value, 1)\n",
        "            return int(action)\n",
        "\n",
        "    # save sample (error,<s,a,r,s'>) to the replay memory\n",
        "    def append_sample(self, state, action, reward, next_state, done):\n",
        "        target = self.model(torch.FloatTensor(state).to(device)).data\n",
        "        old_val = target[0][action]\n",
        "        target_val = self.target_model(torch.FloatTensor(next_state).to(device)).data\n",
        "        if done:\n",
        "            target[0][action] = reward\n",
        "        else:\n",
        "            target[0][action] = reward + self.discount_factor * torch.max(target_val)\n",
        "\n",
        "        error = abs(old_val - target[0][action])\n",
        "\n",
        "        self.memory.add(error, (state, action, reward, next_state, done))\n",
        "\n",
        "    # pick samples from prioritized replay memory (with batch_size)\n",
        "    def train_model(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon -= self.epsilon_decay\n",
        "            #print(self.epsilon)\n",
        "\n",
        "        mini_batch, idxs, is_weights = self.memory.sample(self.batch_size)\n",
        "        mini_batch = np.array(mini_batch).transpose()\n",
        "\n",
        "        states = np.vstack(mini_batch[0])\n",
        "        actions = list(mini_batch[1])\n",
        "        rewards = list(mini_batch[2])\n",
        "        next_states = np.vstack(mini_batch[3])\n",
        "        dones = mini_batch[4]\n",
        "\n",
        "        # bool to binary\n",
        "        dones = dones.astype(int)\n",
        "\n",
        "        # Q function of current state\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        #states = torch.Tensor(states).to(device)\n",
        "        #states = Variable(states).float()\n",
        "        pred = self.model(states)\n",
        "\n",
        "        # one-hot encoding\n",
        "        a = torch.LongTensor(actions).view(-1, 1).to(device)\n",
        "\n",
        "        one_hot_action = torch.FloatTensor(self.batch_size, self.action_size).zero_().to(device)\n",
        "        one_hot_action.scatter_(1, a, 1)\n",
        "\n",
        "        pred = torch.sum(pred.mul(one_hot_action), dim=1)\n",
        "        \n",
        "        # Q function of next state\n",
        "        next_states = torch.FloatTensor(next_states).to(device)\n",
        "        #next_states = torch.Tensor(next_states).to(device)\n",
        "        #next_states = Variable(next_states).float()\n",
        "        next_pred = self.target_model(next_states).data\n",
        "\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        dones = torch.FloatTensor(dones).to(device)\n",
        "\n",
        "        # Q Learning: get maximum Q value at s' from target model\n",
        "        target = rewards + (1 - dones) * self.discount_factor * next_pred.max(1)[0]\n",
        "\n",
        "        errors = torch.abs(pred - target).data.cpu().numpy()\n",
        "        \n",
        "        bellman_error = target - pred\n",
        "        clipped_bellman_error = bellman_error.clamp(-1, 1)\n",
        "        d_error = clipped_bellman_error * -1.0\n",
        "        \n",
        "        # update priority\n",
        "        for i in range(self.batch_size):\n",
        "            idx = idxs[i]\n",
        "            self.memory.update(idx, errors[i])\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # MSE Loss function\n",
        "        #loss = F.mse_loss(pred, target)\n",
        "        #loss.backward()\n",
        "        pred.backward(d_error)\n",
        "\n",
        "        # and train\n",
        "        self.optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Mc63Pz88Jam4",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def set_global_seeds(i):\n",
        "    try:\n",
        "        import torch\n",
        "    except ImportError:\n",
        "        pass\n",
        "    else:\n",
        "        torch.manual_seed(i)\n",
        "    np.random.seed(i)\n",
        "    random.seed(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8cwGNhhoI40c",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def sample_n_unique(sampling_f, n):\n",
        "    \"\"\"Helper function. Given a function `sampling_f` that returns\n",
        "    comparable objects, sample n such unique objects.\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    while len(res) < n:\n",
        "        candidate = sampling_f()\n",
        "        if candidate not in res:\n",
        "            res.append(candidate)\n",
        "    return res\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, size, frame_history_len):\n",
        "        \"\"\"This is a memory efficient implementation of the replay buffer.\n",
        "        The sepecific memory optimizations use here are:\n",
        "            - only store each frame once rather than k times\n",
        "              even if every observation normally consists of k last frames\n",
        "            - store frames as np.uint8 (actually it is most time-performance\n",
        "              to cast them back to float32 on GPU to minimize memory transfer\n",
        "              time)\n",
        "            - store frame_t and frame_(t+1) in the same buffer.\n",
        "        For the typical use case in Atari Deep RL buffer with 1M frames the total\n",
        "        memory footprint of this buffer is 10^6 * 84 * 84 bytes ~= 7 gigabytes\n",
        "        Warning! Assumes that returning frame of zeros at the beginning\n",
        "        of the episode, when there is less frames than `frame_history_len`,\n",
        "        is acceptable.\n",
        "        Parameters\n",
        "        ----------\n",
        "        size: int\n",
        "            Max number of transitions to store in the buffer. When the buffer\n",
        "            overflows the old memories are dropped.\n",
        "        frame_history_len: int\n",
        "            Number of memories to be retried for each observation.\n",
        "        \"\"\"\n",
        "        self.size = size\n",
        "        self.frame_history_len = frame_history_len\n",
        "\n",
        "        self.next_idx      = 0\n",
        "        self.num_in_buffer = 0\n",
        "\n",
        "        self.obs      = None\n",
        "        self.action   = None\n",
        "        self.reward   = None\n",
        "        self.done     = None\n",
        "\n",
        "    def can_sample(self, batch_size):\n",
        "        \"\"\"Returns true if `batch_size` different transitions can be sampled from the buffer.\"\"\"\n",
        "        return batch_size + 1 <= self.num_in_buffer\n",
        "\n",
        "    def _encode_sample(self, idxes):\n",
        "        obs_batch      = np.concatenate([self._encode_observation(idx)[np.newaxis, :] for idx in idxes], 0)\n",
        "        act_batch      = self.action[idxes]\n",
        "        rew_batch      = self.reward[idxes]\n",
        "        next_obs_batch = np.concatenate([self._encode_observation(idx + 1)[np.newaxis, :] for idx in idxes], 0)\n",
        "        done_mask      = np.array([1.0 if self.done[idx] else 0.0 for idx in idxes], dtype=np.float32)\n",
        "\n",
        "        return obs_batch, act_batch, rew_batch, next_obs_batch, done_mask\n",
        "\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample `batch_size` different transitions.\n",
        "        i-th sample transition is the following:\n",
        "        when observing `obs_batch[i]`, action `act_batch[i]` was taken,\n",
        "        after which reward `rew_batch[i]` was received and subsequent\n",
        "        observation  next_obs_batch[i] was observed, unless the epsiode\n",
        "        was done which is represented by `done_mask[i]` which is equal\n",
        "        to 1 if episode has ended as a result of that action.\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size: int\n",
        "            How many transitions to sample.\n",
        "        Returns\n",
        "        -------\n",
        "        obs_batch: np.array\n",
        "            Array of shape\n",
        "            (batch_size, img_c * frame_history_len, img_h, img_w)\n",
        "            and dtype np.uint8\n",
        "        act_batch: np.array\n",
        "            Array of shape (batch_size,) and dtype np.int32\n",
        "        rew_batch: np.array\n",
        "            Array of shape (batch_size,) and dtype np.float32\n",
        "        next_obs_batch: np.array\n",
        "            Array of shape\n",
        "            (batch_size, img_c * frame_history_len, img_h, img_w)\n",
        "            and dtype np.uint8\n",
        "        done_mask: np.array\n",
        "            Array of shape (batch_size,) and dtype np.float32\n",
        "        \"\"\"\n",
        "        assert self.can_sample(batch_size)\n",
        "        idxes = sample_n_unique(lambda: random.randint(0, self.num_in_buffer - 2), batch_size)\n",
        "        return self._encode_sample(idxes)\n",
        "\n",
        "    def encode_recent_observation(self):\n",
        "        \"\"\"Return the most recent `frame_history_len` frames.\n",
        "        Returns\n",
        "        -------\n",
        "        observation: np.array\n",
        "            Array of shape (img_h, img_w, img_c * frame_history_len)\n",
        "            and dtype np.uint8, where observation[:, :, i*img_c:(i+1)*img_c]\n",
        "            encodes frame at time `t - frame_history_len + i`\n",
        "        \"\"\"\n",
        "        assert self.num_in_buffer > 0\n",
        "        return self._encode_observation((self.next_idx - 1) % self.size)\n",
        "\n",
        "    def _encode_observation(self, idx):\n",
        "        end_idx   = idx + 1 # make noninclusive\n",
        "        start_idx = end_idx - self.frame_history_len\n",
        "        # this checks if we are using low-dimensional observations, such as RAM\n",
        "        # state, in which case we just directly return the latest RAM.\n",
        "        if len(self.obs.shape) == 2:\n",
        "            return self.obs[end_idx-1]\n",
        "        # if there weren't enough frames ever in the buffer for context\n",
        "        if start_idx < 0 and self.num_in_buffer != self.size:\n",
        "            start_idx = 0\n",
        "        for idx in range(start_idx, end_idx - 1):\n",
        "            if self.done[idx % self.size]:\n",
        "                start_idx = idx + 1\n",
        "        missing_context = self.frame_history_len - (end_idx - start_idx)\n",
        "        # if zero padding is needed for missing context\n",
        "        # or we are on the boundry of the buffer\n",
        "        if start_idx < 0 or missing_context > 0:\n",
        "            frames = [np.zeros_like(self.obs[0]) for _ in range(missing_context)]\n",
        "            for idx in range(start_idx, end_idx):\n",
        "                frames.append(self.obs[idx % self.size])\n",
        "            return np.concatenate(frames, 0)\n",
        "        else:\n",
        "            # this optimization has potential to saves about 30% compute time \\o/\n",
        "            img_h, img_w = self.obs.shape[2], self.obs.shape[3]\n",
        "            return self.obs[start_idx:end_idx].reshape(-1, img_h, img_w)\n",
        "\n",
        "    def store_frame(self, frame):\n",
        "        \"\"\"Store a single frame in the buffer at the next available index, overwriting\n",
        "        old frames if necessary.\n",
        "        Parameters\n",
        "        ----------\n",
        "        frame: np.array\n",
        "            Array of shape (img_h, img_w, img_c) and dtype np.uint8\n",
        "            and the frame will transpose to shape (img_h, img_w, img_c) to be stored\n",
        "        Returns\n",
        "        -------\n",
        "        idx: int\n",
        "            Index at which the frame is stored. To be used for `store_effect` later.\n",
        "        \"\"\"\n",
        "        # make sure we are not using low-dimensional observations, such as RAM\n",
        "        if len(frame.shape) > 1:\n",
        "            # transpose image frame into (img_c, img_h, img_w)\n",
        "            frame = frame.transpose(2, 0, 1)\n",
        "\n",
        "        if self.obs is None:\n",
        "            self.obs      = np.empty([self.size] + list(frame.shape), dtype=np.uint8)\n",
        "            self.action   = np.empty([self.size],                     dtype=np.int32)\n",
        "            self.reward   = np.empty([self.size],                     dtype=np.float32)\n",
        "            self.done     = np.empty([self.size],                     dtype=np.bool)\n",
        "\n",
        "        self.obs[self.next_idx] = frame\n",
        "\n",
        "        ret = self.next_idx\n",
        "        self.next_idx = (self.next_idx + 1) % self.size\n",
        "        self.num_in_buffer = min(self.size, self.num_in_buffer + 1)\n",
        "\n",
        "        return ret\n",
        "\n",
        "    def store_effect(self, idx, action, reward, done):\n",
        "        \"\"\"Store effects of action taken after obeserving frame stored\n",
        "        at index idx. The reason `store_frame` and `store_effect` is broken\n",
        "        up into two functions is so that one can call `encode_recent_observation`\n",
        "        in between.\n",
        "        Paramters\n",
        "        ---------\n",
        "        idx: int\n",
        "            Index in buffer of recently observed frame (returned by `store_frame`).\n",
        "        action: int\n",
        "            Action that was performed upon observing this frame.\n",
        "        reward: float\n",
        "            Reward that was received when the actions was performed.\n",
        "        done: bool\n",
        "            True if episode was finished after performing that action.\n",
        "        \"\"\"\n",
        "        self.action[idx] = action\n",
        "        self.reward[idx] = reward\n",
        "        self.done[idx]   = done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8rChUbFkIxPb",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        super(NoopResetEnv, self).__init__(env)\n",
        "        self.noop_max = noop_max\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset()\n",
        "        noops = np.random.randint(1, self.noop_max + 1)\n",
        "        for _ in range(noops):\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        return obs\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        super(FireResetEnv, self).__init__(env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def _reset(self):\n",
        "        self.env.reset()\n",
        "        obs, _, _, _ = self.env.step(1)\n",
        "        obs, _, _, _ = self.env.step(2)\n",
        "        return obs\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        super(EpisodicLifeEnv, self).__init__(env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done  = True\n",
        "        self.was_real_reset = False\n",
        "\n",
        "    def _step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert somtimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset()\n",
        "            self.was_real_reset = True\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "            self.was_real_reset = False\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env=None, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super(MaxAndSkipEnv, self).__init__(env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = deque(maxlen=2)\n",
        "        self._skip       = skip\n",
        "\n",
        "    def _step(self, action):\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for _ in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            self._obs_buffer.append(obs)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"Clear past frame buffer and init. to first obs. from inner env.\"\"\"\n",
        "        self._obs_buffer.clear()\n",
        "        obs = self.env.reset()\n",
        "        self._obs_buffer.append(obs)\n",
        "        return obs\n",
        "\n",
        "def _process_frame84(frame):\n",
        "    img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n",
        "    img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
        "    resized_screen = cv2.resize(img, (84, 110),  interpolation=cv2.INTER_LINEAR)\n",
        "    x_t = resized_screen[18:102, :]\n",
        "    x_t = np.reshape(x_t, [84, 84, 1])\n",
        "    return x_t.astype(np.uint8)\n",
        "\n",
        "class ProcessFrame84(gym.Wrapper):\n",
        "    def __init__(self, env=None):\n",
        "        super(ProcessFrame84, self).__init__(env)\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(84, 84, 1))\n",
        "\n",
        "    def _step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        return _process_frame84(obs), reward, done, info\n",
        "\n",
        "    def _reset(self):\n",
        "        return _process_frame84(self.env.reset())\n",
        "\n",
        "class ClippedRewardsWrapper(gym.Wrapper):\n",
        "    def _step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        return obs, np.sign(reward), done, info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q9whOJ5jIuNI",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#@title\n",
        "def wrap_deepmind(env):\n",
        "    assert 'NoFrameskip' in env.spec.id\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    if 'FIRE' in env.unwrapped.get_action_meanings():\n",
        "        env = FireResetEnv(env)\n",
        "    env = ProcessFrame84(env)\n",
        "    env = ClippedRewardsWrapper(env)\n",
        "    return env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZkZE8Rh6P6Ty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "060635ab-60f9-4611-bb4b-43454cbd38dd"
      },
      "cell_type": "code",
      "source": [
        "env = gym.make('PongNoFrameskip-v4')\n",
        "f = open(\"log.txt\", \"a\")\n",
        "f.write(\"\\n====================\\n\")\n",
        "f.close()\n",
        "#env = gym.make('Pong-ram-v0')\n",
        "seed = random.randint(0, 9999)\n",
        "set_global_seeds(seed)\n",
        "env.seed(seed)\n",
        "env = wrap_deepmind(env)\n",
        "\n",
        "action_size = env.action_space.n\n",
        "print(action_size)\n",
        "agent = DQNAgent(action_size)\n",
        "#PATH = \"/content/1_frame_Pong_raw_pixel_linear469.pt\"\n",
        "#agent.model=torch.load(PATH)\n",
        "\n",
        "scores, episodes = [], []\n",
        "\n",
        "print_counter = 0\n",
        "ave_scores = 0.\n",
        "\n",
        "for e in range(EPISODES):\n",
        "    done = False\n",
        "    score = 0\n",
        "\n",
        "    state = env.reset()\n",
        "    state = np.reshape(state, [1, 1, 84, 84])\n",
        "    #state = np.reshape(state, [1, state_size])\n",
        " \n",
        "    while not done:\n",
        "        if agent.render:\n",
        "            env.render()\n",
        "\n",
        "        # get action for the current state and go one step in environment\n",
        "        action = agent.get_action(state)\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        #next_state = cv2.cvtColor(cv2.resize(next_state, (84, 84)), cv2.COLOR_BGR2GRAY)\n",
        "        next_state = np.reshape(next_state, [1, 1, 84, 84])\n",
        "\n",
        "        #next_state = np.reshape(next_state, [1, state_size])\n",
        "        # if an action make the episode end, then gives penalty of -100\n",
        "        #reward = reward if not done or score == 499 else -10\n",
        "        \n",
        "        # save the sample <s, a, r, s'> to the replay memory\n",
        "        agent.append_sample(state, action, reward, next_state, done)\n",
        "        # every time step do the training\n",
        "        if agent.memory.tree.n_entries == agent.train_start:\n",
        "            print(\"Training Started\")\n",
        "        if agent.memory.tree.n_entries >= agent.train_start:\n",
        "            agent.train_model()\n",
        "        score += reward\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            # every episode update the target model to be same with model\n",
        "            agent.update_target_model()\n",
        "            \n",
        "            # every episode, plot the play time\n",
        "            #score = score if score == 500 else score + 10\n",
        "            scores.append(score)\n",
        "            ave_scores += score\n",
        "            episodes.append(e)\n",
        "            pylab.plot(episodes, scores, 'b')\n",
        "            pylab.savefig(\"cartpole_dqn.png\")\n",
        "            \n",
        "            print_counter += 1\n",
        "            if print_counter == 10:\n",
        "                print_counter = 0\n",
        "                print(\"episode:\", e, \" ave_scores:\", ave_scores / 10., \"  memory length:\",\n",
        "                      agent.memory.tree.n_entries, \"  epsilon:\", agent.epsilon)\n",
        "                f = open(\"log.txt\", \"a\")\n",
        "                f.write(\"episode:  \"+str(e)+\"  ave_scores:   \"+str(ave_scores / 10.)+\"  memory length:  \"+str(agent.memory.tree.n_entries)+ \"  epsilon:  \"+ str(agent.epsilon)+\"\\n\")\n",
        "                f.close()\n",
        "                np.savetxt('/content/1_frame_priority_deepmindnet_pong', scores, fmt='%.2f')\n",
        "                ave_scores = 0\n",
        "                PATH = \"/content/1_frame_Pong_raw_pixel_linear\"+str(e)+\".pt\"\n",
        "                torch.save(agent.model, PATH)\n",
        "                \n",
        "                \n",
        "\n",
        "            # if the mean of scores of last 10 episode is bigger than 10\n",
        "            # stop training\n",
        "            if np.mean(scores[-min(10, len(scores)):]) > 18:\n",
        "                torch.save(agent.model, \"1_frame_Pong_raw_pixel_linear\")\n",
        "                sys.exit()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.uint8'>. Please provide explicit dtype.\u001b[0m\n",
            "6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[33mWARN: <class '__main__.ClippedRewardsWrapper'> doesn't implement 'reset' method, which is required for wrappers derived directly from Wrapper. Deprecated default implementation is used.\u001b[0m\n",
            "episode: 9  ave_scores: -20.2   memory length: 9605   epsilon: 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:241: UserWarning: Couldn't retrieve source code for container of type DQN. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "episode: 19  ave_scores: -20.6   memory length: 18349   epsilon: 1.0\n",
            "episode: 29  ave_scores: -20.1   memory length: 27441   epsilon: 1.0\n",
            "episode: 39  ave_scores: -20.5   memory length: 36567   epsilon: 1.0\n",
            "episode: 49  ave_scores: -20.1   memory length: 45681   epsilon: 1.0\n",
            "Training Started\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0lyx8IQ457Bl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " f.write(\"episode:  \"+str(e)+\"  ave_scores:   \"+str(ave_scores / 10.)+\"  memory length:  \",\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hZTmOdPA3d8w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f = open(\"key.txt\", \"a\")\n",
        "i = 1\n",
        "e=1212\n",
        "while i<20:\n",
        "    f.write(str(i)+\"\\n\")\n",
        "    f.write(\"episode:  \"+str(e)+\"  ave_scores:   \")\n",
        "    i +=1 \n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}